{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip3 install tqdm -qq\n",
    "!pip3 install tokenizers==0.13.0 -qq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "commentary = 'commentary.jsonl'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(commentary) as file:\n",
    "    entries = json.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('bible-books-lookup.json') as file:\n",
    "    book_info = json.load(file)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convert Verse References to Lists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_vref(ranges):\n",
    "    for vref in ranges:\n",
    "        # get the key to the value we want to change later\n",
    "        key = next(iter(vref))\n",
    "        references = list(vref.values())\n",
    "        refs = []\n",
    "        \n",
    "        for reference in references:\n",
    "            pattern = r'(?:(\\w+(?:\\s\\w+)?)\\s)?(\\d+):(\\d+)(?:-(\\d+))?'\n",
    "\n",
    "            # Use the re.match() function to find the matches\n",
    "            match = re.match(pattern, reference)\n",
    "\n",
    "            # Extract the book, chapter, and verse from the match\n",
    "            book = match.group(1)\n",
    "            chapter = match.group(2)\n",
    "            verse_start = match.group(3)\n",
    "            verse_end = match.group(4)\n",
    "            \n",
    "            for i in book_info:\n",
    "                if i['name'] == book:\n",
    "                    abbr = i['abbr']\n",
    "\n",
    "            # Determine if it's a single verse or a verse range\n",
    "            if verse_end:\n",
    "                verse_refs = []\n",
    "                verse_range = list(range(int(verse_start), int(verse_end) + 1))\n",
    "                for i in verse_range:\n",
    "                    verse_refs.append(f'{abbr} {chapter}:{i}')\n",
    "            else:\n",
    "                verse_refs = [f'{abbr} {chapter}:{verse_start}']\n",
    "\n",
    "            refs.append(verse_refs)\n",
    "        \n",
    "        vref[key] = refs"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Translate \"Text\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"facebook/nllb-200-3.3B\")\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(\"facebook/nllb-200-3.3B\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate_texts(lang_code):\n",
    "    translations = []\n",
    "    for i in entries:\n",
    "        for j in i['text']:\n",
    "            inputs = tokenizer(j, return_tensors=\"pt\").to(model.device)\n",
    "            translated_tokens = model.generate(\n",
    "                        **inputs, \n",
    "                        forced_bos_token_id=tokenizer.lang_code_to_id[lang_code], \n",
    "                        max_length=30\n",
    "                    )\n",
    "            target_translation = tokenizer.batch_decode(translated_tokens, skip_special_tokens=True)[0]\n",
    "            translations.append(target_translation)\n",
    "            \n",
    "    return translations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for entry in entries:\n",
    "    convert_vref(entry['vrefRanges'])\n",
    "    entry['spa_text'] = translate_texts('spa_Latn')\n",
    "    entry['por_text'] = translate_texts('por_Latn')\n",
    "    entry['fra_text'] = translate_texts('fra_Latn')\n",
    "    entry['ita_text'] = translate_texts('ita_Latn')\n",
    "    entry['arb_text'] = translate_texts('arb_Arab')\n",
    "    print(entry)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "entries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
